IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 64, NO. 4, FEBRUARY 15, 2016

959

Predicting Grades
Yannick Meier, Jie Xu, Onur Atan, and Mihaela van der Schaar, Fellow, IEEE

Abstract—To increase efﬁcacy in traditional classroom courses
as well as in Massive Open Online Courses (MOOCs), automated
systems supporting the instructor are needed. One important
problem is to automatically detect students that are going to
do poorly in a course early enough to be able to take remedial
actions. Existing grade prediction systems focus on maximizing
the accuracy of the prediction while overseeing the importance of
issuing timely and personalized predictions. This paper proposes
an algorithm that predicts the ﬁnal grade of each student in a
class. It issues a prediction for each student individually, when the
expected accuracy of the prediction is sufﬁcient. The algorithm
learns online what is the optimal prediction and time to issue
a prediction based on past history of students’ performance in
a course. We derive a conﬁdence estimate for the prediction
accuracy and demonstrate the performance of our algorithm on a
dataset obtained based on the performance of approximately 700
UCLA undergraduate students who have taken an introductory
digital signal processing over the past seven years. We demonstrate
that for 85% of the students we can predict with 76% accuracy
whether they are going do well or poorly in the class after the
fourth course week. Using data obtained from a pilot course, our
methodology suggests that it is effective to perform early in-class
assessments such as quizzes, which result in timely performance
prediction for each student, thereby enabling timely interventions
by the instructor (at the student or class level) when necessary.
Index Terms—Forecasting algorithms, online learning, grade
prediction, data mining, digital signal processing education.

I. INTRODUCTION

E

DUCATION is in a transformation phase; knowledge
is increasingly becoming freely accessible to everyone
(through Massive Open Online Courses, Wikipedia, etc.) and
is developed by a large number of contributors rather than
by a single author [1]. Furthermore, new technology allows
for personalized education enabling students to learn more
efﬁciently and giving teachers the tools to support each student
individually if needed, even if the class is large [2].
Grades are supposed to summarize in a single number or letter
how well a student was able to understand and apply the knowledge conveyed in a course. Thus it is crucial for students to obtain the necessary support to pass and do well in a class. How-

Manuscript received May 24, 2015; revised October 10, 2015; accepted October 13, 2015. Date of publication October 30, 2015; date of current version
January 18, 2016. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Amir Asif. This research is supported by the US Air Force Ofﬁce of Scientiﬁc Research under the DDDAS
Program.
The authors are with the Department of Electrical Engineering, University
of California, Los Angeles, CA, 90095 USA (e-mail: yannick_meier91@msn.
com).
Color versions of one or more of the ﬁgures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TSP.2015.2496278

ever, with large class sizes at universities and even larger class
sizes in Massive Open Online Courses (MOOCs), which have
undergone a rapid development in the past few years, it has become impossible for the instructor and teaching assistants to
keep track of the performance of each student individually. This
can lead to students failing in a class who could have passed if
appropriate remedial actions had been taken early enough or excellent students not receiving the necessary promotion to beneﬁt
maximally from the course. Remedial or promotional actions
could consist of additional online study material presented to the
student in a personalized and/or automated manner [3]. Hence,
in both ofﬂine and online education, it is of great importance
to develop automated personalized systems that predict the performance of a student in a course before the course is over and
as soon as possible. While in online teaching systems a variety
of data about a student such as responses to quizzes, activity in
the forum and study time can be collected, the available data
in a practical ofﬂine setting are limited to scores in early performance assessments such as homework assignments, quizzes
and midterm exams.
In this paper we focus on predicting grades in traditional
classroom-teaching where only the scores of students from past
performance assessments are available. However, we believe
that our methods can also be applied for online courses such
as MOOCs. We design a grade prediction algorithm that ﬁnds
for each student the best time to predict his/her grade such that,
based on this prediction, a timely intervention can be made if
necessary. Note that we analyze data from a digital signal processing course where no interventions were made; hence, we
do not study the impact of inventions and consider only a single
grade prediction for each student. However, our algorithm can
be easily extended to multiple predictions per student.
A timely prediction exclusively based on the limited data
from the course itself is challenging for various reasons. First,
since at the beginning most students are motivated, the score of
students in early performance assessments (e.g., homework assignments) might have little correlation with their score in later
performance assessments, in-class exams and the overall score.
Second, even if the same material is covered in each year of the
course, the assignments and exams change every year. Therefore, the informativeness of particular assignments with regard
to predicting the ﬁnal grade may change over the years. Third,
the predictability of students having a variety of different backgrounds is very diverse. For some students an accurate prediction can be made very early based on the ﬁrst few performance
assessments. If for example a student shows an excellent performance in the ﬁrst three homework assignments and in the
midterm exam, it is highly likely that he/she will pass the class.
For other students it might take more time to make an equally
accurate prediction. If a student for example performs below
average but not terribly at the beginning, it is risky to predict

1053-587X © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

960

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 64, NO. 4, FEBRUARY 15, 2016

whether he/she is going to pass or fail and, therefore, to decide
whether or not to intervene. This third challenge illustrates the
necessity to make the prediction for each student individually
and not for all at the same time.
The main contributions of this paper can be summarized as
follows.
1) We propose an algorithm that makes a personalized and
timely prediction of the grade of each student in a class.
The algorithm can both be used in regression settings,
where the overall score is predicted, and in classiﬁcation
settings, where the students are classiﬁed into two (e.g.,
do well/poorly) or more categories.
2) We accompany each prediction with a conﬁdence estimate
indicating the expected accuracy of the prediction.
3) We derive a bound for the probability that the prediction
error is larger than a desired value .
4) We exclusively use the scores students achieve in early
performance assessments such as homework assignments
and midterm exams and do not use any other information
such as age, gender or previous GPA. This makes our algorithm applicable in all practical traditional classroom and
online teaching settings, where such information may not
be available.
5) Since the algorithm is learning from past years, the predictions become more accurate when more data from previous
years become available.
6) We demonstrate that the algorithm shows good robustness
if different instructors have taught the course in past years.
7) We analyze real data from an introductory digital signal
processing course taught at UCLA over 7 years and use
the data to experimentally demonstrate the performance of
our algorithm compared to benchmark prediction methods.
As benchmark algorithms we use well known algorithms
such as linear/logistic regression and k-Nearest Neighbors,
which are still a current research topic [4]–[6].
8) Based on our simulations, we suggest a preferred way of
designing courses that enables early prediction and early
intervention. Using data from a pilot course, we demonstrate the advantages of the suggested design.
The rest of the paper is organized as follows. Section II discusses related work in the ﬁeld of grade and GPA prediction
in education. In Section III we introduce notation, deﬁne data
structures, formalize the problem and present the grade prediction algorithm. We analyze the data, describe benchmark
methods and present simulation results including our and benchmark algorithms in Section IV. Finally, we draw conclusions in
Section V.
II. RELATED WORK
Various studies have investigated the value of standardized
tests [7]–[9] admissions exams [10] and GPA in previous programs [8] in predicting the academic success of students in undergraduate or graduate schools. They agree on a positive correlation between these predictors and success measures such as
GPA or degree completion. Besides standardized tests, the relevancy of other variables for predictions of a student’s GPA have
been investigated, usually resulting in the conclusion that GPA
from prior education and past grades in certain subjects (e.g.,
math, chemistry) [11], [12] have a strongly positive correlation

as well. Reference [11] observes that simple linear and more
complex nonlinear (e.g., artiﬁcial neural network) models frequently lead to similar prediction accuracies and concludes that
there is either no complex nonlinear pattern to be found in the
underlying data or the pattern cannot be recognized by their approach. Our simulations support the statement that simple linear
models show a similar accuracy in grade predictions as more
complex methods.
Reference [13] argues that the accuracy of GPA predictions
frequently is mediocre due to different grading standards used
in different classes and shows a higher validity for grade predictions in single classes. Consequently, many works focus on
identifying relationships between a student’s grade in a particular class and variables related to the student [14]–[23]. Relevant factors were found to include the student’s prior GPA [14],
[15], [19]–[21], [23], performance in related courses [20], [21],
[23], previous semester marks [17], performance in entrance
exams [15], performance in early assignments of the class [21],
[23], class attendance [19], self-efﬁcacy [22] and whether the
student is repeating the class [21].
A limitation of the algorithms in the previously discussed papers is that they are difﬁcult to apply in many education scenarios. Frequently, variables related to the student such as performance in related classes, GPA or self-efﬁcacy are not available to the instructor because the data has not been collected
or is not accessible due to privacy reasons. However, the instructor always has access to data he/she collects from his/her
own course, such as the performance of each student in early
homework assignments or midterm exams. This paper, therefore, focuses on predicting the ﬁnal grade based on this easily
accessible data, which is collected anyway by the instructor.
Other works [24]–[30], which also exclusively use data from
the course itself, differ signiﬁcantly from this paper in several
aspects. First, they rely on logged data in online education or
Massive Open Online Course (MOOC) systems such as information about video-watching behavior, time spent on speciﬁc
questions or forum activity. In contrast, our results are applicable to both online and ofﬂine courses, which include some
kind of graded assignments or related feedback from the students during the course. Second, in order for the instructor to be
able to take corrective actions it is of great importance to predict with a certain conﬁdence the performance of students as
early as possible. While our algorithm takes this into account
by deciding for each student individually the best time to make
the prediction using a conﬁdence measure, related works do not
provide a metric indicating the optimal time to predict. Third,
while related works need training data from the course whose
grades they want to predict, we show that we can use training
data from past year classes of the same course. Finally, in contrast to algorithms from related work, which are only shown to
be applicable to classiﬁcation settings (e.g., pass/fail or letter
grade), our algorithm can be used both in regression and classiﬁcation settings.
To make the predictions, related works use various data
mining models such as regression models [14], [26], decision
trees [15]–[18], [25], [26], [30], support vector machines [14],
[23]–[25], neural networks [15], [27], [29], Bayesian classiﬁers
[15], [25], clustering [26] and nearest neighbor techniques [23],
[24], [29], [30].

MEIER et al.: PREDICTING GRADES

961

TABLE I
COMPARISON WITH RELATED WORK

during which the course is taught and is the total number of
performance assessments of each year. For a given year we use
to
index as a representation of th student of the year and
denote the total number of students attending in year . Except
for the rare case that a student retakes the course, the students in
denote the normalized
each year are different. Let
score or grade of student in performance assessment of year
.
The feature vector of th year student after having taken per.
formance assessment is given by
of th year student
The normalized overall score
is the weighted sum of all performance assessments
(1)

Fig. 1. System diagram for a single student.

Table I summarizes the comparison between our paper and
related work investigating and predicting student performance
in a course.
III. FORMALISM, ALGORITHM AND ANALYSIS
In this section we mathematically formalize the problem and
propose an algorithm that predicts the ﬁnal score or a classiﬁcation according to the ﬁnal grade of a student with a given
conﬁdence.
A. Definitions and System Description
Consider a course which is taught for several years with
only slight modiﬁcations. Students attending the course have to
complete performance assessments such as graded homework
assignments, course projects and in-class exams and quizzes
throughout the entire course.1 Our goal is to predict with a
certain conﬁdence the overall performance of a student before
all performance assessments have been taken. See Fig. 1 for a
depiction of the system.
We consider a discrete time model with
and
where denotes the year in which the course is
taught and the point in time in year after the th performance
assessment has been graded. gives the total number of years
1The performance assessments are usually graded by teaching assistants, by
the instructor or even by other students through peer review [31].

where the
denote the weight of performance assessment
so that
. The weights are set by the instructor and
we assume that in each year the number, sequence and weight
of performance assessments is the same. This assumption is reasonable since the content of a course usually does not change
drastically over the years and frequently the same course material (e.g., course book) is used.2 This is especially true in an
introductory course such as the one we investigate in Section IV.
of th year student after perThe residual (overall score)
formance assessment is deﬁned as
(2)
Using this deﬁnition we can write the overall score of th year
student as
(3)
Note that after having taken the performance assessment , the
instructor has access to all the scores up to assignment but
the residual scores
need to be estimated. We denote the
estimate of the residual score for th year student at time
by
and the corresponding estimate of the overall score
. In binary classiﬁcation settings, where the goal is to
by
predict whether a student achieves a letter grade above or below
a certain threshold, we denote the class of th year student by
.
we store the set of feature vectors
For each student
, the set of residuals
and the student’s overall score
2This assumption is made for simplicity. As we discuss in Section IV-B and
show in Fig. 6 we can apply our algorithm to settings where different instructors
using a different number and sequence of performance assessments and using
different weights for each performance assessment teach the course.

962

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 64, NO. 4, FEBRUARY 15, 2016

. All feature vectors from all students of year are given
by
and
denotes all feature
vectors of all completed years. Similarly
and
denote all residuals and overall scores
of all completed years. Let
denote the set of feature vectors and
denote the set of residuals saved after performance assessment
.
B. Problem Formulation
Having introduced notations, deﬁnitions and data structures,
we now formalize the grade prediction problem. We will investigate two different types of predictions. The objective of the
ﬁrst type, which we refer to as regression setting, is to accurately predict the overall score of each student individually in
a timely manner. The second problem, referred to as classiﬁcation setting, aims at making a binary prediction whether the
student will do well or poorly or whether he/she will necessitate additional help or not. Again, the prediction is personalized
and takes timeliness into account. For both types of predictions,
the same algorithm can be used with only slight modiﬁcations,
which we discuss in Section III-D. We will also show that the
binary prediction problem can easily be generalized to a classiﬁcation into three or more classes.
Irrespective of the type of the prediction, the decision for a
th year student consists of two parts. First, we decide after
to predict for the given stuwhich performance assessment
dent and second we determine his/her estimated overall score
or his/her estimated binary classiﬁcation
. At a point in
time of year all scores including the overall scores of all
are known. Thus all feastudents of past years
, residuals
and overall scores
ture vectors
of all completed years are known. Furthermore, the scores
of th year student up to assessment are
known as well and do not have to be estimated. However, to
determine the overall score of the student we need to predict
consisting of performance assesshis/her residual score
since they lie in the future and are unknown.
ments
At time we have to decide for each student of the current year
to predict or whether
whether this is the optimal time
it is better to wait for the next performance assessment. If we
decide to predict, we determine the optimal prediction of the
. Both decisions are made based
overall score
on the feature vector
of the given student and the feature
and residuals
of past students. To devectors
termine the optimal time to predict, we calculate a conﬁdence
indicating the expected accuracy of the prediction for
each student after each performance assessment. The prediction
for a particular student is made as soon as the conﬁdence ex. The problem of
ceeds a user-deﬁned threshold
ﬁnding the optimal prediction time for th year student is formalized as follows:

(4)
The optimization problem results in the optimal prediction time
.

C. Grade Prediction Algorithm, Regression Setting
In this section we propose an algorithm that learns to predict a student’s overall performance based on data from classes
held in past years and based on the student’s results in already
graded performance assessments. We describe the algorithm for
the regression setting and explain the changes needed to use the
algorithm in the classiﬁcation setting in Section III-D.
of the
Since at time we know the scores
considered student from past performance assessments as well
, we only predict the
as the corresponding weights
and calculate the prediction of the overall score
residual
with (3). To make its prediction for the current residual of a
, the algorithm ﬁnds all feastudent with feature vector
ture vectors from similar students of past years and their cor. We deﬁne the similarity of students
responding residuals
through their feature vectors. Two feature vectors
are similar if
where
is a distance metric
and is a parameter. For two
deﬁned on the feature space
and
from different feature
feature vectors
) the distance metric is not deﬁned since we
spaces (i.e.,
only need to determine distances within a single feature space.
Different feature spaces can have different deﬁnitions of the distance metric; we are going to deﬁne the distance metrics we use
with rain Section IV-B. We deﬁne a neighborhood
as all feature vectors
dius of feature vector
with
.
denote the random variable representing the residual
Let
score after performance assessment .
denotes the
probability distribution over the residual score for a student with
denotes the student’s exfeature vector at time and
denote the probability distribupected residual score. Let
. Intuitively
tion of the students over the feature space
is the fraction of students with feature vector at time . Note
that the distributions
and
are not sampling
distributions but unknown underlying distributions. We assume
that the distributions do not change over the years.
We deﬁne the probability distribution of the students in a
with center
and radius as
neighborhood

where is the indicator function. Intuitively
is the
with feature
fraction of students in neighborhood
be the random variable representing
vector . Let
after
the residual score of students in neighborhood
having taken performance assessment . The distribution of
is given by

We denote the true expected value of the residual scores
after assignment of students in a particular neighborhood by
. Note that

MEIER et al.: PREDICTING GRADES

963

Our estimation of the true expected residual of students
is given by
within a particular neighborhood

(5)
denotes the residual after time of the student
where
with feature vector . For notational simplicity, we use
to denote the estimated
expectation. In the following we are going to derive how conﬁdent we are in the estimation of the residual score based on a
and how we use this conﬁdence
given neighborhood
to both select the optimal radius of the neighborhood and to decide when to predict.
Intuitively, if the feature vectors after performance assessof contain a lot of informament in a neighborhood
, past students with feature vectors in
tion about the residual
this neighborhood should have had similar residuals. Hence, the
of the students
variance of the residuals
in the neighborhood should be small. To mathematically support
in a neighborhood
this intuition, we consider the residuals
of feature vector with distribution
. For
any conﬁdence interval the probability that the absolute difference between the unknown residual
of the student with
feature vector and the expected value of the residual distribuin his/her neighborhood is smaller than can be
tion
bounded by

(6)
This statement directly follows from Chebyshev’s inequality.
We conclude that the lower the variance of the residual
distribution in the neighborhood, the more conﬁdent we are that
will be close to
. Since both the
the true residual
and the variance
expected value
of the distribution are unknown, we estimate the two values
through the sample mean from (5) and the sample variance
given by
(7)
In the following we use

to

to
denote the variance and
denote the sample variance of the residual distribution in neigh. From the law of large number it follows that
borhood
the sample mean and the sample variance converge to the true
. We
expected value and the true variance for
will provide a bound for the probability that the prediction error
is larger than a given value in the theorem below. Given a desired conﬁdence interval , we deﬁne the conﬁdence on the prediction of the residual as

.
after each performance assessment ,
different neighborhoods
considers
with user-deﬁned radii
and
according to our
chooses the best neighborhood
.
conﬁdence measure
to denote the best
In the following we use
neighborhood. Let
To estimate
our algorithm

(9)
denote the estimated residual of the best neighborhood at time
and
denotes the corresponding estimated overall score
(10)
If the conﬁdence bound for the best neighborhood
is above a given threshold
,
the algorithm returns the ﬁnal prediction of the overall score
for the considered student.
If the conﬁdence is below the threshold, we wait for the next
performance assessment and start the next iteration. Fig. 2 illustrates the neighborhood selection process. Algorithm 1 provides
a formal description of the grade prediction algorithm in pseudocode.
Algorithm 1: Grade Prediction Algorithm, Regression Setting
Input: All and from past years,
, number
and
of neighborhoods
radii
Output: Predictions for the overall scores of the students
1: for all years do
2: for all performance assessments do
3:
for all current-year students for whom the ﬁnal
prediction has not been made do
then
4:
if
5:
Calculate
according to (1)
as ﬁnal prediction for student
6:
Return
7:
end if
neighborhoods with radii
8:
Create
9:
for all neighborhoods do
with (5)
10:
Estimate residual
11:
Compute
with (7)
with (8)
12:
Compute
13:
end for
14:
Find
15:
if
then
with (3)
16:
Compute
as ﬁnal prediction for student
17:
Return
18:
end if
19:
Add
and
to database
20:
end for
21: end for
of year according to (2)
22: Calculate all
to database
23: Add all
24: end for

(8)
Using this conﬁdence measure the radius of the optimal
is given by
neighborhood after performance assessment

To conclude the discussion of the grade prediction algorithm
in the regression setting, we derive a bound for the probability
that the prediction error is larger than a value . Before we state

964

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 64, NO. 4, FEBRUARY 15, 2016

Proof: See Appendix.
This theorem illustrates two important aspects of algorithm
1. First, we see that for a given neighborhood the accuracy of
our predictions increases with an increasing number of neighbors. Hence, our algorithm learns the best predictions online as
the knowledge base is expanded after each year, when the feature vectors and results from the past-year students are added
to the database. In Section IV-D1 we show that this learning
can be experimentally illustrated with our data from the digital signal processing course taught at UCLA. Second, the term
shows that the prediction accuracy will be
higher if the variance of the residuals in a neighborhood is small.
With increasing time we expect this variance to decrease since
we have more information about the students and we expect the
students in a neighborhood to be more similar and achieve similar (residual) scores.
Note that it is possible to restrict the data kept in the knowledge base to recent years, which allows the algorithm to adapt
faster to slowly changing students and to changes in the course.
D. Grade Prediction Algorithm, Classification Setting

Fig. 2. Illustration of the neighborhood selection process.

the theorem, we introduce some further notations. Let
denote the index of the neighborhood with the smallest variance
of residuals for the student with feature vector at time
(11)
is not necessarily equal to
, the index
Note that
of the neighborhood with the highest conﬁdence chosen by our
algorithm, since the conﬁdence deﬁned in (8) is calculated with
and not with
the known sample variance of residuals
the unknown true variance
used in (11).
denotes the index of the neighborhood
Similarly
with the second highest conﬁdence.

Let
denote the difference between the standard deviations of the residual distribution of neighborhoods
and
(12)
Theorem: Without loss of generality we assume that all
scores
are normalized to the range [0, 1]. Consider the
of the overall score of th year student with
prediction
feature vector made by algorithm 1. The probability that the
absolute error the prediction exceeds is bounded by

In the binary classiﬁcation setting we predict the overall
score analogously to the regression setting and then determine
with
the class by comparing the predicted overall score
. To illustrate how we ﬁnd
let us
a threshold score
assume that we want to predict whether a student does well
) or does poorly (letter grades
). To
(letter grades
, we ﬁnd the average
of all students from
determine
and the average
of all
past years who received a
students from past years who achieved a
. Subsequently,
as
. The predicted
we deﬁne
classiﬁcation
of th year student is then given by
(13)
We are more conﬁdent in the classiﬁcation not only if the variance of the neighbor-scores is small, which is the metric we used
for the conﬁdence in the regression setting, but also if the disbetween the predicted
tance
is the
score and the threshold score is large. Note that
.
estimate of the overall score based on neighborhood
Because of this intuition we use a modiﬁed conﬁdence
(14)
to decide whether to make the ﬁnal prediction in binary classiﬁshould only inﬂuence whether
cation settings. Since
the ﬁnal prediction is made for a given neighborhood but not
the neighborhood selection process, we still use the unmodiﬁed
conﬁdence from (8) to select the optimal neighborhood.
In summary, four changes have to be made to algorithm 1
to make it applicable to binary classiﬁcation settings. First,
has to be determined/updated at the beginning of each new year.
after line 16 according to (13). Third,
Second, we calculate
we return
at line 17 instead of
. Fourth, we use the modiﬁed conﬁdence
according to (14) in line 15 instead of the
unmodiﬁed conﬁdence . We use the unmodiﬁed conﬁdence
from (8) in line 14.
The described binary classiﬁcation algorithm can easily be
generalized to a larger number of categories. In a classiﬁcation

MEIER et al.: PREDICTING GRADES

965

with

categories, we deﬁne
threshold values
and determine in which of the score inthe predicted
tervals
of a student lies. The index of the interval
overall score
corresponds to the classiﬁcation of the student. In this general
classiﬁcation setting, the modiﬁed conﬁdence from (14) can be
to the nearest
used as well by deﬁning as the distance of
threshold value.
We discuss the performance of the proposed algorithm 1 in
both regression and classiﬁcation settings in Section IV-D.

. For this purpose, we extract
and from algorithm
. We
2 for a large number of different conﬁdence thresholds
which is optimal with
then select the conﬁdence threshold
respect to optimization problem (15) and determine the corresponding prediction time . To make the grade predictions for
we use the learned conﬁdence threshold
as input
year
to prediction algorithm 1. Since there are no training data avail, the algorithm uses a user-deﬁned starting
able yet at year
for the grade predictions of the ﬁrst year. Algorithm
value
2 summarizes the learning algorithm in pseudocode.

E. Confidence-Learning Prediction Algorithm
Besides the radii of the neighborhoods , the only parameter to be chosen by the user in algorithm 1 is the desired con. Since for an instructor it is more natural
ﬁdence threshold
and practical to specify a desired prediction accuracy or error
directly rather than the conﬁdence threshold, we show in this
section how to automatically learn the appropriate conﬁdence
threshold to achieve a certain prediction performance and what
consequences this has on the average prediction time. We will
discuss a possible way of choosing the radii of the neighborhoods in Section IV-B.
Formally we deﬁne the problem as follows. Let
denote the proportion of current year students for which
the grade prediction algorithm working with conﬁdence
has predicted the overall score by time
threshold
(performance assessment)
.
is the minimum
percentage of current year students whose grade the user wants
denotes the
to predict with a speciﬁed accuracy.
average absolute prediction error up to time for the given
is the maximum error the user is willing to
conﬁdence.
is the time necessary to predict the grade
tolerate.
for proportion of all students of the class using conﬁdence
. Please note that since the variables
and
threshold
are dependent, we can only independently specify two of the
four variables. If we for example specify to predict all
students with zero error
, the algorithm will have to
wait until the end of the course when the overall score is known
and will use maximum conﬁdence
. Without
making any assumptions on the dependence of the variables
might lead to the same
of each other, multiple pairs
.
speciﬁed pair
Our goal is, therefore, to learn from past data the th year
and corresponding conﬁdence
estimate of the minimal time
threshold
necessary to achieve the desired share
of
students predicted and the desired maximum average prediction
. This is formally deﬁned as:
error

Algorithm 2: Conﬁdence-Learning Prediction Algorithm

(15)
Note that while the goal of optimization problem (4) is to ﬁnd
the minimum time to predict the overall score of a particular student with a desired conﬁdence, this problem (15) aims at ﬁnding
the minimum time by which the overall scores of a speciﬁc percentage of all students can be predicted with a desired maximum
error.
At a given year we solve this optimization problem using
a brute force approach using the all available data from years

Input:
number

, all
and radii

and from past years,
of neighborhoods

Output: Predictions
and
for all years
1: for all years do
then
2: if
and
according (15) by running
3:
Find
algorithm 1 with various
4:
Return
and
5: end if
to predict and
6: Use algorithm 1 with
return the grades of current year students
7: end for
IV. EXPERIMENTS
In this section, we present the data, discuss details of the application of algorithm 1 to our dataset, illustrate the functioning
of the algorithm and evaluate its performance by comparing
it against other prediction methods in both regression and binary classiﬁcation settings. Due to space limitations, we will not
show experimental results for classiﬁcation settings with more
than two categories.
A. Data Analysis
Our experiments are based on a dataset from an undergraduate digital signal processing course (EE113) taught at UCLA
over the past 7 years. The dataset contains the scores from all
performance assessments of all students and their ﬁnal letter
grades. The number of students enrolled in the course for a given
year varied between 30 and 156, in total the dataset contains the
scores of approximately 700 students. Each year the course consists of 7 homework assignments, one in-class midterm exam
taking place after the third homework assignment, one course
project that has to be handed in after homework 7 and the ﬁnal
exam. The duration of the course is 10 weeks and in each week
one performance assessments takes place. The weights of the
performance assessments are given by: 20% homework assignments with equal weight on each assignment, 25% midterm
exam, 15% course project and 40% ﬁnal exam.3 Fig. 3(a) shows
the distribution of the letter grades assigned over the 7 years. We
observe that on average is the grade the instructor assigned
most frequently. was assigned second most and third most
3As we explain in footnote 2 and show in Section IV-D2, our algorithm can
also be applied to settings where the number and weights of performance assessments change over the years.

966

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 64, NO. 4, FEBRUARY 15, 2016

Fig. 3. Data analysis: (3a) shows the distribution of letter grades for all years. (3b) and (3c) present the sample Pearson correlation coefﬁcient between individual
(homework assignment ), M (midterm exam), F
performance assessments and the overall (3b) or ﬁnal exam (3c) score. Note that we use the abbreviations
(ﬁnal exam) and O (overall score) in the ﬁgures. (a) Grade distribution, (b) Correlation coefﬁcients overall score, (c) Correlation coefﬁcients ﬁnal score.

frequently. Surprisingly, however, the distribution varies drastically over the years; in year 1 for example only 18.75% received
a while in year 6 the frequency was 38.9%.
To understand the predictive power of the scores in different
performance assessments, Fig. 3(b) shows the sample Pearson
correlation coefﬁcient between all performance assessments and
the overall score. We make several important observations from
this graph. First, on average the ﬁnal exam has the strongest
correlation to the overall score, followed by the midterm exam.
This is not surprising, since the ﬁnal contributes 40% and the
midterm contributes 25% to the overall score. Second, the score
from the course project on average does not have a higher correlation with the overall score than the homework assignments despite the fact that it accounts for 15% of the overall score. Third,
all homework assignments have similar correlation coefﬁcients.
Fourth, the correlation between the individual performance assessments and the overall score varies greatly over the years.
This indicates that predicting student scores based on training
data from past years might be difﬁcult.
Since all performance assessments are part of the overall
score and, therefore, a high correlation is expected, it is also informative to consider the correlation between the performance
assessments and the ﬁnal exam shown in Fig. 3(c). It is interesting to observe that still the midterm exam shows, besides
the overall score, the highest correlation with the ﬁnal exam. A
possible explanation for this is that both the midterm and ﬁnal
are in-class exams while the other performance assessments
are take-home.

the scores is needed for several reasons. First, the instructor-deﬁned maximum score in a particular performance assessment
may differ greatly across years and since we use data from past
years to predict the performance of students in a given year, we
need to make the data across years comparable. Second, also the
difﬁculty of individual performance assessments might be different across years, homework 2 might for instance be very easy
in year 2 so that almost everyone achieves the maximum score
and very difﬁcult in year 3 so that few achieve half of the maximum score. The normalization according to (16) eliminates this
bias by transforming the absolute scores of a student to scores
relative to his/her classmates of the same year. Note that algorithm 1 does not require a speciﬁc normalization and it does not
matter that the normalized scores according to (16) will not be
in the interval [0, 1] as assumed in Section III for simplicity.
Second, we use feature vectors that simply contain the scores
of all performance assessments student has taken up to time
in the order they occurred
. To incorporate the fact that students who have performed similarly in
a performance assessment with a lot of weight should be nearer
to each other in the feature space than students that have had
similar scores in a performance assessment (e.g., homework assignment) with low weight, we use a weighted metric to calculate the distance between two feature vectors. We deﬁne the
as
distance of two feature vectors

B. Our Algorithm
In this section we discuss four important details of the application of algorithm 1 to the dataset from the undergraduate
digital signal processing course.
in our
First, the rule we use to normalize all scores
dataset is given by

is the weight
where is the length of the feature vectors,
denotes entry of feature
of performance assessment and
vector .
Third, rather than specifying the radii of the neighborhoods
to consider as an input, as suggested in the pseudo-code of algorithm 1, we automatically adapt the radii of the neighborhoods such that they contain a certain number of neighbors.
Since the sample variance gets more accurate with an increasing
number of samples, we refrain from considering neighborhoods
with only 2 neighbors. Therefore, the smallest radius considis the minimal radius such that the neighborhood inered
cludes 3 neighbors. For subsequent neighborhoods the minimal
radius is chosen such that the neighborhood includes at least

(16)
is the original score of the student,
is the
where
sample mean of all th year student’s original scores in peris the standard deviation of all
formance assessment and
th year student’s original overall scores. A normalization of

(17)

MEIER et al.: PREDICTING GRADES

967

one neighbor more than the previous neighborhood. Formally,
we deﬁne the selection of the radii recursively as

TABLE II
CASE STUDY: ILLUSTRATIVE EXAMPLE

(18)
Fourth, to be able to apply our algorithm in settings in which
structure of the course (e.g., the number, weight and sequence
of assessments) changes across years, we need to pre-process
the data from past years. In particular, the data from past years
is pre-processed so that the number and sequence of in-class
and take-home assessments is the same as in the current year.
In addition, to identify the most similar students, it is important
that the performance assessments that cover the same topic are
at the same place of the sequence/feature vector and, therefore,
are compared to each other. Consequently, we might have to
pre-process the data from past years even if the total number
of performance assessments is the same. We use two different
types of modiﬁcations to pre-process data from past years.
Modiﬁcation 1 applies to cases where a topic of the course
was tested with a larger number of performance assessments in
a past year than in a current year. For example, consider a signal
processing course which contained two homework assignments
on the Fast Fourier Transform in year 1 but the same topic was
covered in only one homework assignment in year 2. In this
case, the two performance assessments on the same topic from
assessments
year 1 are combined to a single assessment. If
is
are combined, the score of the combined assessment
of the past assesscalculated based on the weights
according to
ments with scores
(19)
Modiﬁcation 2 applies to the case where a topic of the course
was tested with a lower number of performance assessments in a
past year than in a current year. In this case the past-year performance assessment on this topic is duplicated. Note that through
duplication this performance assessment gets more weight in the
process of selecting similar students. This is desired because the
instructor probably uses more performance assessments to test
a certain topic because he thinks that this topic is very important
and hence it will be informative in terms of predicting the grade.
Finally, if necessary the sequence of performance assessments from the past years is reordered to match the sequence of
performance assessments from the current year. The reordering
has to be done so that the performance assessments on the same
topic are at the same position of the sequence/feature vector in
both years. Additionally, in-class assessments should only be
compared to in-class assessments and take-home assessments
should only be compared to take-home assessments. After this
pre-processing of past-year data, the standard Algorithm 1 can
be applied to make the predictions. Note that our algorithm always uses the weights of the current-year course to ﬁnd students
similar to the student for whom it needs to issue a personalized
grade prediction and does not consider the weights that were
used in past-year courses for the various assessments. The
grade predictions for the current-year course are usually made
based on data from several past-year courses. The data for each
of the past years might have to be pre-processed separately.

C. Benchmarks
We compare the performance of our algorithm against ﬁve
different prediction methods.
student has achieved in the most
• We use the score
alone to predict the
recent performance assessment
overall grade.
• A second simple benchmark makes the prediction based
student has achieved up to
on the scores
performance assessment taking into account the corresponding weights of the performance assessments.
• The -Nearest Neighbors algorithm with 7 neighbors. This
number provided the best results with training data from
the ﬁrst year.
• Linear regression using the ordinary least squares (OLS)
ﬁnds the least squares optimal linear mapping between the
scores of ﬁrst performance assessments and the overall
score.
• In classiﬁcation settings we use logistic regression instead
of linear regression.
• Support vector machines (SVMs) are used in the classiﬁcation setting.
The advantage of the method we use in our algorithm over
linear and logistic regression is that being a nearest neighbor
method, it is able to recognize certain patterns such as trends
in the data that are missed in linear/logistic regression where a
single parameter per performance assessment has to ﬁt all students. In contrast, our algorithm is able to detect such patterns
if there have been students in the past who have shown similar
patterns.
Table II illustrates this through a case study extracted from
the UCLA undergraduate digital signal processing course
data. We present cases from a simulation where we predicted
)
whether students are going to do well (letter grade
) and consider the students
or do poorly (letter grade
for which our algorithm decided to predict after the midterm
exam. The table shows 3 students whom logistic regression
classiﬁed wrongly while our algorithm made the accurate
prediction. In columns 2-4 we present the scores the students
achieved up to the midterm exam and the last column shows
the true classiﬁcation of the students. These cases are typical
examples of settings where our algorithm outperforms logistic
regression. Student 1 and 2 both showed a good performance
in homework assignment 1. However, in later assignments and
especially at the midterm exam their performance successively
deteriorated, an indication that the students might do poorly
the class if they or the instructor and teaching assistants do not
take corrective actions. Our algorithm is likely to have learned
such patterns from past data and predicts the students to do
poorly. On average, however, their performance in the ﬁrst four
performance assessments is still about average and, therefore,
logistic regression predicts that the students will do well. For
student 3 the situation is the other way around.

968

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 64, NO. 4, FEBRUARY 15, 2016

Fig. 4. Performance comparison of different prediction methods.

Fig. 5. Illustration of learning from past data: Error of grade predictions for
year 7 depending on training data.

D. Results
In this section we evaluate the performance of our algorithm
1 in different settings and compared to benchmarks in both regression and the classiﬁcation tasks.
As a performance measure in the regression setting, we use
the average of the absolute values of the prediction errors .
Since we normalized the overall score to have zero mean and a
standard deviation of 1, directly corresponds to the number
of standard deviations the predictions on average are away from
the true values. The overall performance measure in classiﬁcation settings is the accuracy of the classiﬁcation. Furthermore,
we use the quantities precision, recall and false positive/negative rate besides accuracy to measure performance. Please note
that positive in our case means that the student does poorly.
1) Performance Comparison With Benchmarks in Regression
Setting: Having discussed the various performance measures,
we ﬁrst address the regression setting. Fig. 4 visualizes the performance of the algorithm we presented in Section III-C and
of benchmark methods. We generated Fig. 4 by predicting the
overall scores of all students from years 2–7. To make the prediction for year , we used the entire data from years 1 to
to learn from. Unlike our algorithms, the benchmark methods
do not provide conditions to decide after which performance
assessment the decision should be made. Therefore, for benchmark methods we speciﬁed the prediction time (performance
assessment) for an entire simulation and repeated the exper; the results are plotted in Fig. 4.
iment for all
To generate the curve of our algorithm 1, we ran simulations
and for each threshold
using different conﬁdence thresholds
we determined and the performance assessment (time) after
which the prediction was made on average.
Irrespective of the prediction method, Fig. 4 shows the
trade-off between timeliness and accuracy; the later we predict the more accurate our prediction gets. From the curve
for the prediction using a single performance assessment
we infer that there is a low correlation between homework
assignments/course project and the overall score and a high
correlation between the in-class assessments (midterm and ﬁnal
exam) and the overall score. This observation is congruent with
the correlation analysis from Section IV-A. If the prediction is

made early, before the midterm, all methods (except the prediction using a single performance assessment) lead to similar
prediction errors. We observe that while the error decreases
approximately linearly for our algorithm, the performance of
benchmark methods steeply increases after the midterm and
the ﬁnal but stays approximately constant during the rest of the
time. The reason for this is that we obtained the points of the
curve for our algorithm by averaging the prediction time of all
students. Therefore, the point of the curve above the midterm
was not generated by predicting after the midterm for all
students; some predictions were made earlier, some later. If on
average the prediction is made after homework 4, our algorithm
shows a signiﬁcantly smaller error than benchmark methods
outperforming linear regression by up to 65%.
2) Learning Across Years and Instructors in Regression Setting: Consider Fig. 5 demonstrating the performance increase
of our algorithm when more data to learn from become available. To generate the ﬁgure, we used our algorithm to predict
the overall scores of all 7th year students for different conﬁdence thresholds. The curves in dashed lines stem from simulations using only one of the years 1–5 as training data and the
solid magenta curve uses all years 1–5 to learn from. We observe that the prediction performance strongly depends on the
training data and differs if different years are used. Most importantly, the performance is highest irrespective of the average
prediction time if the combination of the data from all 5 years
is used. This shows that our algorithm is able to learn and improves its predictions over time.
The undergraduate digital signal processing course is taught
twice a year by three different instructors at UCLA. While we
used only the data from one instructor in the previous plots,
Fig. 6 investigates the situation when we predict the grades for
a class of instructor 1 based exclusively on past data from a
different instructor 2. In practice this happens when a new instructor takes over a course previously taught by someone else.
It is interesting to see whether our grade prediction still works
well in this setting. A good performance is not self-evident for
several reasons. Different instructors might set a different focus
concerning the knowledge imparted, they might use a different

MEIER et al.: PREDICTING GRADES

Fig. 6. Illustration of learning across instructors: Error of grade predictions for
year 7 depending on training data.

textbook and they might prefer different styles of homework
assignments and in-class exams. Furthermore, the structure of
the course, e.g., the number and sequence of homework assignments, the time when the midterm exam takes place, the weights
of performance assessments and whether a course project and
quizzes exist, might change drastically. To generate Fig. 6, we
predicted the overall score for the year 7 class of instructor 1
based on two different sets of previous data. The solid blue curve
was generated by using the data from the classes in years 1–5
from the same instructor 1 as training data. To obtain the dashed
red curve, we used the data from classes in years 1–5 from instructor 2 to learn from. While the predictions using training
data from the same instructor are slightly more accurate, the performance with training data from a different instructor is still
very satisfying, showing a good robustness of our algorithm
with respect to different instructors. For the subsequent results
we again exclusively use data from one instructor.
3) Performance Comparison With Course Containing Early
Quizzes in Regression Setting: The results in both the data analysis section (Fig. 3(b)) and Section IV-D1 (Fig. 4) indicate that
scores in in-class exams are much better predictors of the overall
score than homework assignments. To verify this, we consider
two consecutive years of the UCLA course EE103, which contains four in-class quizzes in course weeks 2, 4, 6 and 8 instead of a midterm. Fig. 7 visualizes that, starting from the ﬁrst
quiz in week 2, indeed our algorithm is able to predict the same
percentage of the students with an up to 22% smaller cumulative average prediction error by a certain week. We generated
Fig. 7 by using algorithm 1 to predict for both courses the overall
scores of the students in a particular year based on data from the
previous year. Note that for the course with quizzes, the increase
in the share of students predicted is larger in weeks that contain
quizzes than in weeks without quizzes. This supports the thesis
that quizzes are good predictors as well.
According to this result, it is desirable to design courses with
early in-class exams. This enables a timely and accurate grade
prediction based on which the instructor can intervene if necessary.

969

Fig. 7. Comparison of prediction time and accuracy between the UCLA course
EE113, which contains a midterm exam, and the UCLA course EE103, which
contains four in-class quizzes instead of a midterm exam. Note that the tick
above the plot stand for quiz/homework and that for EE103
labels
there are weeks in which both a homework and a quiz take place.

Fig. 8. Performance comparison between our algorithm and logistic regression
using accuracy, precision and recall for binary do well/poorly classiﬁcation.

4) Performance Comparison With Benchmarks in Classification Setting: The performances in the binary classiﬁcation settings are summarized in Fig. 8. Since logistic regression turns
out to be the most challenging benchmark in terms of accuracy
in the classiﬁcation setting, we do not show the performance
of the other benchmark algorithms for the sake of clarity. The
goal was to predict whether a student is going to do well, still
, or do poorly, dedeﬁned as letter grades equal to or above
. Again, to generate
ﬁned as letter grades equal to or below
the curves for the benchmark method, logistic regression, we
speciﬁed manually when to predict. For our algorithm we again
averaged the prediction times of an entire simulation and varied
to obtain different points of the curve. Up to homework 4, the
performance of the two algorithms is very similar, both showing

970

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 64, NO. 4, FEBRUARY 15, 2016

Fig. 9. Cumulative prediction time, accuracy, false positive and false negative
.
error rates for a binary do well/poorly classiﬁcation with ﬁxed

a high prediction accuracy even with few performance assessments. Starting from homework 4, our algorithm performs signiﬁcantly better, with an especially drastic improvement of recall. It is interesting to see that even with full information, the algorithms do not achieve a 100% prediction accuracy. The reason
for this is that the instructor did not use a strict mapping between
overall score and letter grade and the range of overall scores that
lead to a particular letter grade changed slightly over the years.
5) Decision Time and Accuracy in Classification Setting: To
better understand when our algorithm makes decisions and with
what accuracy, consider Fig. 9. We again investigate binary do
well/poorly classiﬁcations as discussed above. The red curve
shows (square markers) for what share of the total number of
students the algorithm makes the prediction by a speciﬁc point
in time. The remaining curves show different measures of cumulative performance. We can for example see that by the midterm
exam we classify 85% of the students with an accuracy of 76%.
These timely predictions are desirable since the earlier the prediction is made the more time an instructor has to take corrective action. The cumulative accuracy stays almost constant
around 80% irrespective of the prediction time. We believe that
the reason for this is that thanks to the conﬁdence threshold, the
easy decisions are made early and harder decisions are made
later. Consequently, the expected accuracy of all predictions remains more or less constant irrespective of the prediction time.
V. CONCLUSION
In this paper we develop an algorithm that allows for a timely
and personalized prediction of the ﬁnal grades of students exclusively based on their scores in early performance assessment
such as homework assignments, quizzes or midterm exams.
Using data from an undergraduate digital signal processing
course taught at UCLA, we show that the algorithm is able to
learn from past data, that it outperforms benchmark algorithms
with regard to accuracy and timeliness both in classiﬁcation
and regression settings and that the predictions are robust even
when the course is taught by different instructors.
We show that in-class exams are better predictors of the
overall performance of a student than homework assignments.
Hence, designing courses to have early in-class evaluations

enables timely identiﬁcation of students who, with a high
probability, would do poorly without intervention and enables
remedial actions to be adopted at an early stage.
Our algorithm can easily be generalized to include context
data from students such as their prior GPA or demographic data.
If applied exclusively to MOOCs, the in-course data used for the
predictions could be extended for example by the responses of
students to multiple-choice questions, their forum activity, the
course material they studied or the time they spent studying online. Another direction of future work is to apply our algorithm
in practice and investigate to what extent the performance of
students can be improved by a timely intervention based on the
grade predictions. In this context, our algorithm could be extended to make multiple predictions for each student to monitor
the trend in the predicted grade after an intervention.
One example for an intervention would be that the instructor
provides additional study material to students with a low predicted grade. Alternatively, teaching assistants could spend additional time with these selected students to go through important topics again. In a MOOC setting, the intervention could take
place in a fully automated way, for example by presenting the
students additional study material in a personalized way using
techniques discussed in [3]. To make students aware of their performance, they could be asked to predict their own overall grade
and as a comparison the instructor could disclose the prediction
of our algorithm to the students.
APPENDIX
In this Appendix, we proof the theorem from Section III-C.
Before we start with the proof, we discuss some preliminary
results.
Fact 1. (Chernoff-Hoeffding Bound): Let
be
independent and bounded random variables with range
and expected value . Let
denote
the sample mean of the random variables. Then, for all

Proof: A proof of Fact 1 can be found in Hoeffding’s paper
[32].
Fact 2. (Empirical Bernstein Bound): Let
and
be independent and bounded random random
variables with range [0, 1] and variance
.
denotes
the -sample mean
and
denotes the
-sample variance
. Then, the
following inequality bounds the probability that the error of
the sample standard deviation, which is the square root of the
sample variance, is larger than a given value

can be derived.
Proof: See [33]for a proof of Fact 2.
Lemma 1: Let
denote the index of the neighborhood
selected by our algorithm for the student with feature vector
at time and
is given by (11).
denotes the total
number of neighborhoods our algorithm considers and
is

MEIER et al.: PREDICTING GRADES

971

given by (12). We can bound the probability that our algorithm
chooses the wrong neighborhood by

In the following we separate these three error sources and derive
a bound for each one.
We have

(20)
Proof: Consider:

If the estimation error of the standard deviation is smaller than
for all neighborhoods

our algorithm chooses the optimal neighborhood
fore, we get

. There-

where (b) follows from (9), (c) is the law of total probability
and (d) and (e) both follow from the fact that
.
Lemma 1 provides a bound for the second term. Therefore, we
focus on the ﬁrst term

where

where
is the union bound and
Proof of Theorem: Note that

follows from Fact 2.

where
follows from (3) and (10).
There are three sources of error in the prediction of an overall
score of algorithm 1:
1) The wrong neighborhood size may be selected due to inaccurate approximations of the true residual score variances
of the neighborhoods through the sample variance.
2) If the optimal neighborhood is selected, the sample mean
of the residual scores in the neighborhood may not be a
good approximation of their true mean.
3) Even if the optimal neighborhood is selected and the
sample mean equals the true mean, the residual score of
the considered student may be different from the mean of
the residual score distribution.

follows from the triangle inequality, the fact that
and the
union bound. The bound for the ﬁrst term in step
follows
from Chebyshev’s inequality and the bound for the second term
follows from the Chernoff-Hoeffding Bound from Fact 1.
Including the second term again and using Lemma 1 we get

which concludes the proof.
REFERENCES
[1] R. Baraniuk, “Open education: New opportunities for signal processing,” in Plenary Speech, 2015 IEEE Int. Conf. Acoustics, Speech
and Signal Processing (ICASSP), 2015.
[2] Openstax college [Online]. Available: http://openstaxcollege.org/, accessed: 2015-05-07
[3] C. Tekin, J. Braun, and M. van der Schaar, “Etutor: Online learning
for personalized education,” in Proc. 2015 IEEE Int. Conf. Acoustics,
Speech and Signal Processing (ICASSP), 2015.

972

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 64, NO. 4, FEBRUARY 15, 2016

[4] G. Marjanovic and V. Solo, “
sparsity penalized linear regression
with cyclic descent,” IEEE Trans. Signal Process., vol. 62, no. 6, pp.
1464–1475, Mar. 15, 2014.
[5] S. Marano, V. Matta, and P. Willett, “Nearest-neighbor distributed
learning by ordered transmissions,” IEEE Trans. Signal Process., vol.
61, no. 21, pp. 5217–5230, Nov. 1, 2013.
[6] G. Mateos, J. A. Bazerque, and G. B. Giannakis, “Distributed sparse
linear regression,” IEEE Trans. Signal Process., vol. 58, no. 10, pp.
5262–5276, May 15, 2010.
[7] N. R. Kuncel and S. A. Hezlett, “Standardized tests predict graduate
students success,” Science, vol. 315, pp. 1080–1081, 2007.
[8] E. Cohn, S. Cohn, D. C. Balch, and J. Bradley, “Determinants of undergraduate gpas: Sat scores, high-school GPA and high-school rank,”
Econ. Ed. Rev., vol. 23, no. 6, pp. 577–586, 2004.
[9] E. R. Julian, “Validity of the medical college admission test for predicting medical school performance,” Acad. Med., vol. 80, no. 10, pp.
910–917, 2005.
[10] P. A. Gallagher, C. Bomba, and L. R. Crane, “Using an admissions
exam to predict student success in an ADN program,” Nurse Ed., vol.
26, no. 3, pp. 132–135, 2001.
[11] W. L. Gorr, D. Nagin, and J. Szczypula, “Comparative study of artiﬁcial neural network and statistical models for predicting student grade
point averages,” Int. J. Forecasting, vol. 10, no. 1, pp. 17–34, 1994.
[12] N. T. Nghe, P. Janecek, and P. Haddawy, “A comparative analysis
of techniques for predicting academic performance,” in Proc. 37th
Annu. Frontiers In Education Conf.—Global Engineering: Knowledge
Without Borders, Opportunities Without Passports (FIE’07), 2007,
pp. T2G–7, IEEE.
[13] R. D. Goldman and R. E. Slaughter, “Why college grade point average
is difﬁcult to predict,” J. Ed. Psychol., vol. 68, no. 1, p. 9, 1976.
[14] S. Huang and N. Fang, “Predicting student academic performance in an
engineering dynamics course: A comparison of four types of predictive
mathematical models,” Comput. Ed., vol. 61, pp. 133–145, 2013.
[15] E. Osmanbegović and M. Suljić, “Data mining approach for predicting
student performance,” Econ. Rev., vol. 10, no. 1, 2012.
[16] B. K. Baradwaj and S. Pal, “Mining educational data to analyze
students’ performance,” 2012 [Online]. Available: http://arxiv.org/
abs/1201.3417, unpublished
[17] S. K. Yadav, B. Bharadwaj, and S. Pal, “Data mining applications: A
comparative study for predicting student’s performance,” 2012 [Online]. Available: http://arxiv.org/abs/1202.4815, unpublished
[18] A. B. E. D. Ahmed and I. S. Elaraby, “Data mining: A prediction for
student’s performance using classiﬁcation method,” World J. Comput.
Appl. Technol., vol. 2, no. 2, pp. 43–47, 2014.
[19] P. Cortez and A. M. G. Silva, “Using data mining to predict secondary
school student performance,” 2008.
[20] L. H. Werth, “Predicting sttudent performance in a beginning computer
science class,” in Proc. 17th SIGCSE Technical Symp. Computer Science Education (SIGCSE ’86), 1986, vol. 18, no. 1, pp. 138–143.
[21] J. L. Turner, S. A. Holmes, and C. E. Wiggins, “Factors associated with
grades in intermediate accounting,” J. Accounting Ed., vol. 15, no. 2,
pp. 269–288, 1997.
[22] A. Y. Wang and M. H. Newlin, “Predictors of web-student performance: The role of self-efﬁcacy and reasons for taking an on-line
class,” Compute. Human Behav., vol. 18, no. 2, pp. 151–163, 2002.
[23] S. Kotsiantis, C. Pierrakeas, and P. Pintelas, “Predicting students’performance in distance learning using machine learning techniques,”
Appl. Artif. Intell., vol. 18, no. 5, pp. 411–426, 2004.
[24] C. G. Brinton and M. Chiang, “MOOC performance prediction via
clickstream data and social learning networks,” in Proc. IEEE 34th INFOCOM, 2015, to be published.
[25] C. Romero, M.-I. López, J.-M. Luna, and S. Ventura, “Predicting students’ ﬁnal performance from participation in on-line discussion forums,” Comput. Ed., vol. 68, pp. 458–472, 2013.
[26] M. I. Lopez, J. Luna, C. Romero, and S. Ventura, “Classiﬁcation via
clustering for predicting ﬁnal marks based on student participation in
forums.,” presented at the Int. Conf. Educational Data Mining, 2012.
[27] M. D. Calvo-Flores, E. G. Galindo, M. P. Jiménez, and O. P. Pineiro,
“Predicting students marks from moodle logs using neural network
models,” Current Devel. Technol.-Assisted Ed., vol. 1, pp. 586–590,
2006.
[28] D. Garcıa-Saiz and M. Zorrilla, “A promising classiﬁcation method for
predicting distance students performance,” Proc. 5th Int. Conf. Educational Data Mining, pp. 206–207, 2012.

[29] C. Romero, S. Ventura, P. G. Espejo, and C. Hervás, “Data mining
algorithms to classify students,” in Proc. Int. Conf. Educational Data
Mining, 2008, pp. 8–17.
[30] B. Minaei-Bidgoli, D. A. Kashy, G. Kortemeyer, and W. F. Punch,
“Predicting student performance: An application of data mining
methods with an educational web-based system,” in Proc. 33rd Annu.
Frontiers in Education Conf., 2003, vol. 1, pp. T2A–13.
[31] Y. Xiao, F. Dörﬂer, and M. van der Schaar, “Incentive design in peer
review: Rating and repeated endogenous matching,” in Proc. Allerton
Conf. , 2014.
[32] W. Hoeffding, “Probability inequalities for sums of bounded random
variables,” J. Amer. Stat. Assoc., vol. 58, no. 301, pp. 13–30, 1963.
[33] A. Maurer and M. Pontil, “Empirical bernstein bounds and sample variance penalization,” in Proc. Int. Conf. Learning Theory, 2009.

Yannick Meier received the B.Sc. degree in information technology and electrical engineering from ETH
Zurich, Zurich, Switzerland (Swiss Federal Institute
of Technology Zurich) in 2014.
In 2014 and 2015 he conducted research visits at
University of Pennsylvania, Philadelphia, PA, USA
and at University of California, Los Angeles, CA,
USA. He is currently pursuing the M.Sc. degree in
information technology and electrical engineering at
ETH Zurich.

Jie Xu is an Assistant Professor in the Department
of Electrical and Computer Engineering at the
University of Miami. He received his B.S. and M.S.
degrees in electronic engineering from Tsinghua
University in China in 2008 and 2010, respectively,
and a Ph.D. degree in electrical engineering from
University of California, Los Angeles (UCLA)
in 2015. Dr. Xu’s research interests are in game
theory and learning theory with applications to
education, communication, signal processing and
network security. He received the Distinguished
Ph.D. Dissertation Award in Signals & Systems at UCLA.

Onur Atan received the B.Sc. degree in electrical
engineering from Bilkent University, Ankara,
Turkey in 2013 and the M.Sc. degree in electrical
engineering from the University of California, Los
Angeles in 2014. He is currently pursuing the Ph.D.
degree in electrical engineering at the University
of California, Los Angeles. He received the best
M.Sc. thesis award in Electrical Engineering at the
University of California, Los Angeles. His research
interests include online learning and multi-armed
bandit problems and their applications to medical
informatics and education.

Mihaela van der Schaar (F’09) is Chancellor’s
Professor in the Electrical Engineering Department
at UCLA. Her research interests include machine
learning for medical informatics and education,
online learning, stream mining, networks, network
science, social networks and game theory. She received numerous awards, including the NSF Career
Award, three IBM Faculty Awards, several best
paper awards including the Darlington Best Paper
Award. She has also 33 U.S. patents.

