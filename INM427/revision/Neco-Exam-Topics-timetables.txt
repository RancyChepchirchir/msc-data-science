Neural Computing
Re-sit examination period: 12-23 August 2019 - resit date published July 19th 17:00h
August 2019 Examination Timetable (published 09:30 02/08/2019)
https://studenthub.city.ac.uk/__data/assets/excel_doc/0003/476715/2018-9-PRDR-Exam-Timetable-published-V3-02.08.2019.xlsx
INM427	Neural Computing	Wed 21/08/2019	10:00	02:00	 D222



Topic											Lecture	Covered

Model of Neuron (Haykin, 2nd edition, p.10-23)  						OK
Perceptron (Haykin, p.117-120) 									OK
Perceptron’s learning algorithm (Haykin, p.135-143, 175-178) 					OK
Multilayer perceptron (feedforward nets) (Haykin, p.156-161)					OK 
Backpropagation (Haykin, p.161-175, 226-232) 							OK
Problem of local minimum, adding momentum, etc. (Haykin, p.191-198) 				OK
Regularization (Haykin, p.218-222) 								OK
Learning evaluation and generalization (Haykin, p.205-209, 213-218)				OK 
Self-organising maps (Haykin, p.443-466) 						5	OK							
Support Vector Machines (Haykin, p.318-324, 329-339, lecture notes, A. Ng notes) 	6	OK ish	
Hopfield networks (Haykin, p.50-66, 680-696) 						5	OK
Gibbs sampling (Haykin, p.545-550, 561-562) 						6	OK
Boltzmann machines, Generative models (Haykin, p.558-560, 562-574) 			6	OK
Restricted Boltzmann machines (Lecture notes)						6, 7	OK	
Contrastive Divergence (Lecture notes) 							7	OK	
Deep learning (Lecture notes) 								7	OK
Recurrent networks (Haykin, p.732-741)							7 
Convolutional nets (lecture notes)							7 
Autoencoders (lecture notes) 								7
Backpropagation through Time (Haykin, p. 751-756, lecture notes)			7


	* Self-organising maps - Key equations

1. Competition phase

In the competition phase, the neuron with weight vector closest (minimum distance) to input vector wins the competition.

i(x) =  arg minj || x - Wj ||^2 

2. cooperative phase

* Neighbourhood concept

hi,j = (di,j^2/2*sigma)


3. Adaptation phase

deltaWji = n * hi,j ( SUMj 


	* Support Vector Machines - Key equations


	* Hopfield networks - key equations

1. Storage phase - one shot learning

Wij = 1/N * SUM as mu goes from 1 to M of Ksi index MU i times Ksi index Mu j

Where M is the number of memories and N is the size of memory vector.


2. Energy function

E = -1/2 SUM as i goes from one to N times SUM as j goes from 1 to N of Weight index i, j times activation of i times activation of j.


	* Contrastive Divergence - key equations



	* Gibbs Sampling - key equations 










