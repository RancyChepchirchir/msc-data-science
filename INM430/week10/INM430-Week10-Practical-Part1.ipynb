{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM430 Week 10 Practical - Part 1\n",
    "Scrape London Datastore  \n",
    "Here we build an inventory of what is available and where  \n",
    "\n",
    "Part 1 of 3 \n",
    "1. Get LDS column names\n",
    "2. Get Health data column names  \n",
    "3. Cross reference and determine scope  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def getLDSDownloadLinksPageCount():\n",
    "    # get the number of London Datastore pages we can scrape\n",
    "    # using the same general format from week 09\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk/dataset\"\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # looking for the paging links found near footer\n",
    "    linkList = soup.find_all('li', attrs={'class': 'dp-search__pagelink'})\n",
    "    # all being well, the list will look like this (where each line is a list element)\n",
    "    \n",
    "    #<li class=\"dp-search__pagelink dp-search__pagelink--disabled\"><span>«</span></li>\n",
    "    #<li class=\"dp-search__pagelink dp-search__pagelink--active\"><span>1</span></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=2\">2</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=3\">3</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=4\">4</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><span>...</span></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=78\">78</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=2\">»</a></li>\n",
    "    \n",
    "    # The line we are interested in is the next to last (page 78), number 78 being\n",
    "    # the text property of the link (a href attribute), which is the 6th element of\n",
    "    # the linkList list starting from index 0\n",
    "    try:\n",
    "        iPagenums = linkList[6].text\n",
    "    except:\n",
    "        # string data type for consistency\n",
    "        iPagenums = \"0\"\n",
    "\n",
    "    return int(iPagenums)\n",
    "\n",
    "def getLDSDownloadLinks(iPagenum):\n",
    "    # each page number will have a number of links,\n",
    "    # with a label (name) and a url (href)\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk/dataset?page=\" + str(iPagenum)\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # look for h3 headers\n",
    "    linkList = soup.find_all('h3', attrs={'class': 'dp-searchresult__heading'})\n",
    "    # our return list\n",
    "    results = []\n",
    "    for linkListItem in linkList:\n",
    "        try:\n",
    "            linkHeader = linkListItem.find('a', attrs={'class': \"dp-searchresult__heading-link\"})\n",
    "            name = linkHeader.text\n",
    "            href = linkHeader['href']\n",
    "            ldslinks = {\n",
    "                \"name\" : name,\n",
    "                \"href\" : href,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(\"Error - no links found\")\n",
    "        results.append(ldslinks)\n",
    "    return results\n",
    "\n",
    "def getLDSFileDownloadLinks(href):\n",
    "    # get the file download links - pdf, xls, etc, decide later what to do\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk\" + href\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    download_links = soup.find_all('div', attrs={'class': 'dp-resource__indented'})\n",
    "    results = []\n",
    "    for download_link in download_links:\n",
    "        try:\n",
    "            link = download_link.find('a', attrs={'class': 'dp-resource__format'})\n",
    "            fileurl = link['href']\n",
    "            links = {\n",
    "                \"fileurl\" : fileurl,\n",
    "            }\n",
    "            results.append(links)\n",
    "        except:\n",
    "            print(\"Error occured parsing file download links\")\n",
    "    # return a list of dictionaries\n",
    "    return results\n",
    "\n",
    "def makeDir(path):\n",
    "    # create directory if required\n",
    "    from pathlib import Path\n",
    "    # remove leading forward slash\n",
    "    if(path[0] == '/'):\n",
    "        path = path[1:]\n",
    "    p = Path(path)\n",
    "    if(p.exists() == False):\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "# TODO download file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the work\n",
    "# 1. get the number of pages\n",
    "iPagenums = getLDSDownloadLinksPageCount()\n",
    "# initialise our links dictionary\n",
    "links = []\n",
    "# 2. Get the links\n",
    "for i in range(1, iPagenums + 1):\n",
    "    links.extend(getLDSDownloadLinks(i))\n",
    "# 3. Create a new entry in our links dictionary, \n",
    "#    consisting of another dictionary with all the available files for download (pdf, xls, etc)\n",
    "for i in range (0, len(links)):\n",
    "    links[i]['fileurls'] = getLDSFileDownloadLinks(links[i]['href'])\n",
    "# 4. Download all the files and save to local /dataset/<label> to keep local path\n",
    "#    aligned with remote path\n",
    "# NB might need to insert random delays between downloads to mask scraping activity a little\n",
    "# 5. Open all the excel files, get the column headers and create a name cloud to get things moving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
