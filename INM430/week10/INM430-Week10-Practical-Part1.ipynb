{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM430 Week 10 Practical - Part 1\n",
    "Scrape London Datastore  \n",
    "Here we build an inventory of what is available and where  \n",
    "\n",
    "Part 1 of 3 \n",
    "1. Get LDS column names\n",
    "2. Get Health data column names  \n",
    "3. Cross reference and determine scope  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def getLDSDownloadLinksPageCount():\n",
    "    # get the number of London Datastore pages we can scrape\n",
    "    # using the same general format from week 09\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk/dataset\"\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # looking for the paging links found near footer\n",
    "    linkList = soup.find_all('li', attrs={'class': 'dp-search__pagelink'})\n",
    "    # all being well, the list will look like this (where each line is a list element)\n",
    "    \n",
    "    #<li class=\"dp-search__pagelink dp-search__pagelink--disabled\"><span>«</span></li>\n",
    "    #<li class=\"dp-search__pagelink dp-search__pagelink--active\"><span>1</span></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=2\">2</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=3\">3</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=4\">4</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><span>...</span></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=78\">78</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=2\">»</a></li>\n",
    "    \n",
    "    # The line we are interested in is the next to last (page 78), number 78 being\n",
    "    # the text property of the link (a href attribute), which is the 6th element of\n",
    "    # the linkList list starting from index 0\n",
    "    try:\n",
    "        iPagenums = linkList[6].text\n",
    "    except:\n",
    "        # string data type for consistency\n",
    "        iPagenums = \"0\"\n",
    "\n",
    "    return int(iPagenums)\n",
    "\n",
    "def getLDSDownloadLinks(iPagenum):\n",
    "    # each page number will have a number of links,\n",
    "    # with a label (name) and a url (href)\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk/dataset?page=\" + str(iPagenum)\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # look for h3 headers\n",
    "    linkList = soup.find_all('h3', attrs={'class': 'dp-searchresult__heading'})\n",
    "    # our return list\n",
    "    results = []\n",
    "    for linkListItem in linkList:\n",
    "        try:\n",
    "            linkHeader = linkListItem.find('a', attrs={'class': \"dp-searchresult__heading-link\"})\n",
    "            name = linkHeader.text\n",
    "            href = linkHeader['href']\n",
    "            ldslinks = {\n",
    "                \"name\" : name,\n",
    "                \"href\" : href,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(\"Error - no links found\")\n",
    "        results.append(ldslinks)\n",
    "    return results\n",
    "\n",
    "def getLDSFileDownloadLinks(href):\n",
    "    # get the file download links - pdf, xls, etc, decide later what to do\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk\" + href\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    download_links = soup.find_all('div', attrs={'class': 'dp-resource__indented'})\n",
    "    results = []\n",
    "    for download_link in download_links:\n",
    "        try:\n",
    "            link = download_link.find('a', attrs={'class': 'dp-resource__format'})\n",
    "            fileurl = link['href']\n",
    "            links = {\n",
    "                \"fileurl\" : fileurl,\n",
    "            }\n",
    "            results.append(links)\n",
    "        except:\n",
    "            # TODO add href to error message\n",
    "            print(\"Error occured parsing file download links\")\n",
    "    # return a list of dictionaries\n",
    "    return results\n",
    "\n",
    "def makeDir(path):\n",
    "    # create directory if required\n",
    "    from pathlib import Path\n",
    "    # remove leading forward slash\n",
    "    if(path[0] == '/'):\n",
    "        path = path[1:]\n",
    "    p = Path(path)\n",
    "    if(p.exists() == False):\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "def jsonifier(links, toplevel):\n",
    "    import json\n",
    "    jsonified = \"{\\\"\" + toplevel + \"\\\":\" + json.dumps(links) + \"}\"\n",
    "    # to keep return type consistent with xmlifier\n",
    "    return str.encode(jsonified)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London datastore scraper started: 2018-11-29 22:35:14\n",
      "Downloaded 80 page links\n",
      "Error occured parsing file download links\n",
      "Error occured parsing file download links\n",
      "Downloaded page link files\n",
      "Wrote LDS download links json file to disk.\n",
      "London datastore scraper ended: 2018-11-29 22:46:58\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Download London Datastore .xls files\n",
    "# Timestamp start\n",
    "now = datetime.datetime.now()\n",
    "print (\"London datastore scraper started:\", now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "# 1. get the number of pages\n",
    "iPagenums = getLDSDownloadLinksPageCount()\n",
    "# initialise our links dictionary\n",
    "links = []\n",
    "# 2. Get the links\n",
    "maxi = 0\n",
    "for i in range(1, iPagenums + 1):\n",
    "    maxi = i\n",
    "    links.extend(getLDSDownloadLinks(i))\n",
    "print(\"Downloaded\", maxi, \"page links\")\n",
    "# 3. Create a new entry in our links dictionary, \n",
    "#    consisting of another dictionary with all the available files for download (pdf, xls, etc)\n",
    "for i in range (0, len(links)):\n",
    "    links[i]['fileurls'] = getLDSFileDownloadLinks(links[i]['href'])\n",
    "print(\"Downloaded page link files\")\n",
    "# 3.5 Save json file to filesystems, store, load tomorrow, rebuild list of dictionaries and carry on\n",
    "myjsonfile = jsonifier(links, \"links\")\n",
    "f = open('lds-links.json', 'wb')\n",
    "f.write(myjsonfile)\n",
    "print(\"Wrote LDS download links json file to disk.\")\n",
    "f.close()\n",
    "# Timestamp start\n",
    "now = datetime.datetime.now()\n",
    "print (\"London datastore scraper ended:\", now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "# 4. Download all the files and save to local /dataset/<label> to keep local path\n",
    "#    aligned with remote path\n",
    "# NB might need to insert random delays between downloads to mask scraping activity a little\n",
    "# 5. Open all the excel files, get the column headers and create a name cloud to get things moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could we save the list as a json file?\n",
    "links = []\n",
    "links.extend(getLDSDownloadLinks(i))\n",
    "for i in range (0, len(links)):\n",
    "    links[i]['fileurls'] = getLDSFileDownloadLinks(links[i]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/download/average-house-prices/f01b1cc7-6daa-4256-bd6c-94d8c83ee000/land-registry-house-prices-borough.xls\n",
      "/download/average-house-prices/b1b0079e-698c-4c0b-b8c7-aa6189590ca4/land-registry-house-prices-borough.csv\n",
      "/download/average-house-prices/fb8116f5-06f8-42e0-aa6c-b0b1bd69cdba/land-registry-house-prices-ward.xls\n",
      "/download/average-house-prices/59be940c-ffb8-426d-a833-6146ea77de5c/land-registry-house-prices-ward.csv\n",
      "/download/average-house-prices/fab83691-9c7e-4e53-97e8-564a010a56ce/land-registry-house-prices-MSOA.xls\n",
      "/download/average-house-prices/bdf8eee7-41e1-4d24-90ce-93fe5cf040ae/land-registry-house-prices-MSOA.csv\n",
      "/download/average-house-prices/af75f04c-5a21-4ab8-bf7c-aa57b42a84eb/land-registry-house-prices-LSOA.xls\n",
      "/download/average-house-prices/9a92fbaf-c04e-498a-9f8c-6c85f280817e/land-registry-house-prices-LSOA.csv\n",
      "/download/london-reservoir-levels/5ef2b35c-44d1-4255-ae59-5a599bf4642e/london-reservoir-levels.xls\n",
      "/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xls\n",
      "/download/household-projection-research-outputs/c5130058-f2d2-43bc-8944-b8395d4593a5/research_outputs_2016_based_central_trend_household_projections.xlsx\n",
      "/download/household-projection-research-outputs/19af13dc-bb73-4928-be98-8826460c5e8a/household%20projection%20research%20outputs.pdf\n",
      "/download/projections/fdcf026a-638c-4411-abfe-0cd5ffe78f5e/housing_led_2016_base.xlsx\n",
      "/download/projections/b9daabe6-6dd3-4082-adcf-a6b458ef4945/central_trend_2016_base.xlsx\n",
      "/download/projections/ba9e0d6d-b139-44bc-b6ae-11246d9831d1/households_central_trend_2016_base.xlsx\n",
      "/download/projections/c9ce96cc-bf56-4da9-94b5-a3c6a1c441ea/gla_2016_based_projections_central.zip\n",
      "/download/projections/aa4c5515-23a0-4c60-954d-620a4f5f7fac/long_term_trend_2016_base.xlsx\n",
      "/download/projections/e9e9c1b2-b9f4-4134-9000-5269e12dde1a/households_long_term_trend_2016_base.xlsx\n",
      "/download/projections/30631488-b358-4237-9c61-26093347c389/gla_2016_based_projections_long_term.zip\n",
      "/download/projections/e44a3ef1-5043-42d8-adba-0407c34b3ff7/short_term_trend_2016_base.xlsx\n",
      "/download/projections/508c0617-5d1d-400e-9ffd-7c3a45c10af5/households_short_term_trend_2016_base.xlsx\n",
      "/download/projections/1b231ee5-0d01-487c-9d8d-3b602aae03e2/gla_2016_based_projections_short_term.zip\n",
      "/download/projections/0b73a55d-fb0a-4dfc-b23f-2b4c1ab35975/ward_housing_led_2016_base.xlsx\n",
      "/download/projections/c1c67f17-f231-460e-813f-9e98ff76b059/msoa_housing_led_2016_base.xlsx\n",
      "/download/projections/2c3e993e-c915-4a75-9904-ce826ab5d2a3/2016-based%20ethnic%20group%20projections%20(housing-led).xlsx\n",
      "/download/projections/ac09149c-c6b4-4288-a593-5e657626bfa0/2016-based%20ethnic%20group%20projections%20(central%20trend).xlsx\n",
      "/download/projections/506af2c5-f735-4758-a176-e2236bf598c1/experimental_employment-led_projections.xlsx\n",
      "/download/london-plan-business-improvement-districts/dbf869d9-ef24-4af9-862a-314bd8ae8268/20181008.zip\n",
      "/download/london-plan-business-improvement-districts/b67d1c1e-5b9f-423c-b2ff-8fc5e4bf3a00/business_improvement_districts_tab.zip\n",
      "/download/london-plan-business-improvement-districts/654cd9b2-1f2f-411d-9ade-b91f3a649cd3/business_improvement_districts.zip\n",
      "/download/incidents-occuring-around-diwali-halloween---bonfire-night/2881218a-5ded-480b-b226-40328abaef3a/Incidents%20During%20Bonfire%20Night%20Diwali%20And%20Halloween.xlsx\n",
      "/download/workplace-employment-publicprivate-sector-borough/b2771a31-d155-4b80-852c-9d21c110c980/workplace-employment-sector-borough.xls\n",
      "/download/communal-heating-consumer-survey-report/e1e4b59d-242d-4fc5-9a7b-23b2c4efebc7/Communal%20heating%20consumer%20survey%20results.xlsx\n",
      "/download/communal-heating-consumer-survey-report/634f0008-1666-482d-bd7c-22dd51cfc830/Communal%20heating%20consumer%20survey%20results.pdf\n",
      "/download/communal-heating-consumer-survey-report/13475a35-7cd4-4e54-af7d-60c9b35a1030/communal_heating_consumer_survey_final.pdf\n",
      "/download/migration-indicators/0db19902-5013-42af-972d-0e5481d7ac44/Long%20term%20international%20migration.xlsx\n",
      "/download/migration-indicators/b56fba6e-99ec-4ad5-bafb-2e1107d67170/LTIM%20citizenship.xlsx\n",
      "/download/migration-indicators/b149c9a1-e5f9-404f-8160-83e0e29d650d/LTIM%20London%20LA%20level(1).xlsx\n",
      "/download/migration-indicators/e17a6ebe-2aa9-424c-bea3-2817f6879995/LTIM%20reason.xlsx\n",
      "/download/migration-indicators/bd3999e4-a8e3-4fb1-8ed3-64d144f1f9cd/National%20Insurance%20numbers%20issued%20to%20overseas%20nationals.xlsx\n",
      "/download/migration-indicators/e138b1ab-f276-422d-8695-03f3e2500790/New%20migrant%20GP%20registrations.xlsx\n",
      "/download/migration-indicators/ec11de0b-3e13-4406-9f5e-161fbe198ba9/Short%20term%20migration.xlsx\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, len(links)):\n",
    "    #print(links[i]['name'], links[i]['href'])\n",
    "    for link in links[i]['fileurls']:\n",
    "        # download .xls\n",
    "        print(link['fileurl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote city courses json file to disk.\n"
     ]
    }
   ],
   "source": [
    "myjsonfile = jsonifier(links, \"links\")\n",
    "f = open('ldslinks.json', 'wb')\n",
    "f.write(myjsonfile)\n",
    "print(\"Wrote city courses json file to disk.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate links rebuilding from json file\n",
    "#import json\n",
    "#f = open('ldslinks.json', 'r')\n",
    "#data = json.load(f)\n",
    "#mylinks = data['links']\n",
    "f.close()\n",
    "#for i in range (0, len(mylinks)):\n",
    "    #print(links[i]['name'], links[i]['href'])\n",
    "#    for link in mylinks[i]['fileurls']:\n",
    "        # download .xls\n",
    "#        print(link['fileurl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
