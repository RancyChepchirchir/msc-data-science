{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM430 Week 10 Practical - Part 1\n",
    "Scrape London Datastore  \n",
    "Here we build an inventory of what is available and where  \n",
    "\n",
    "Part 1 of 3 \n",
    "1. Get LDS column names\n",
    "2. Get Health data column names  \n",
    "3. Cross reference and determine scope  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def getLDSDownloadLinksPageCount():\n",
    "    # get the number of London Datastore pages we can scrape\n",
    "    # using the same general format from week 09\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk/dataset\"\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # looking for the paging links found near footer\n",
    "    linkList = soup.find_all('li', attrs={'class': 'dp-search__pagelink'})\n",
    "    # all being well, the list will look like this (where each line is a list element)\n",
    "    \n",
    "    #<li class=\"dp-search__pagelink dp-search__pagelink--disabled\"><span>«</span></li>\n",
    "    #<li class=\"dp-search__pagelink dp-search__pagelink--active\"><span>1</span></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=2\">2</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=3\">3</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=4\">4</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><span>...</span></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=78\">78</a></li>\n",
    "    #<li class=\"dp-search__pagelink\"><a href=\"/dataset?page=2\">»</a></li>\n",
    "    \n",
    "    # The line we are interested in is the next to last (page 78), number 78 being\n",
    "    # the text property of the link (a href attribute), which is the 6th element of\n",
    "    # the linkList list starting from index 0\n",
    "    try:\n",
    "        iPagenums = linkList[6].text\n",
    "    except:\n",
    "        # string data type for consistency\n",
    "        iPagenums = \"0\"\n",
    "\n",
    "    return int(iPagenums)\n",
    "\n",
    "def getLDSDownloadLinks(iPagenum):\n",
    "    # each page number will have a number of links,\n",
    "    # with a label (name) and a url (href)\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk/dataset?page=\" + str(iPagenum)\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # look for h3 headers\n",
    "    linkList = soup.find_all('h3', attrs={'class': 'dp-searchresult__heading'})\n",
    "    # our return list\n",
    "    results = []\n",
    "    for linkListItem in linkList:\n",
    "        try:\n",
    "            linkHeader = linkListItem.find('a', attrs={'class': \"dp-searchresult__heading-link\"})\n",
    "            name = linkHeader.text\n",
    "            href = linkHeader['href']\n",
    "            ldslinks = {\n",
    "                \"name\" : name,\n",
    "                \"href\" : href,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(\"Error - no links found\")\n",
    "        results.append(ldslinks)\n",
    "    return results\n",
    "\n",
    "def getLDSFileDownloadLinks(href):\n",
    "    # get the file download links - pdf, xls, etc, decide later what to do\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://data.london.gov.uk\" + href\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    download_links = soup.find_all('div', attrs={'class': 'dp-resource__indented'})\n",
    "    results = []\n",
    "    for download_link in download_links:\n",
    "        try:\n",
    "            link = download_link.find('a', attrs={'class': 'dp-resource__format'})\n",
    "            fileurl = link['href']\n",
    "            links = {\n",
    "                \"fileurl\" : fileurl,\n",
    "            }\n",
    "            results.append(links)\n",
    "        except:\n",
    "            # TODO add href to error message\n",
    "            print(\"Error occured parsing file download links for href =\", href)\n",
    "    # return a list of dictionaries\n",
    "    return results\n",
    "\n",
    "def checkPath(path):\n",
    "    # check if path or file exist\n",
    "    from pathlib import Path\n",
    "    retval = False\n",
    "    # remove leading forward slash\n",
    "    if(path[0] == '/'):\n",
    "        path = path[1:]\n",
    "    p = Path(path)\n",
    "    if(p.exists() == True):\n",
    "        retval = True\n",
    "    return retval\n",
    "\n",
    "def makeDir(path):\n",
    "    # create directory if required\n",
    "    from pathlib import Path\n",
    "    # remove leading forward slash\n",
    "    if(path[0] == '/'):\n",
    "        path = path[1:]\n",
    "    p = Path(path)\n",
    "    if(p.exists() == False):\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "def jsonifier(links, toplevel):\n",
    "    import json\n",
    "    jsonified = \"{\\\"\" + toplevel + \"\\\":\" + json.dumps(links) + \"}\"\n",
    "    # to keep return type consistent with xmlifier\n",
    "    return str.encode(jsonified)\n",
    "\n",
    "def checkXls(downloadlink):\n",
    "    isXls = False\n",
    "    filetype = downloadlink[-4:]\n",
    "    filetypes = ['.xls', 'xlsx']\n",
    "    if(filetype in filetypes):\n",
    "        isXls = True\n",
    "    return isXls\n",
    "\n",
    "def downloadFile(linkspath, downloadpath):\n",
    "    # expected values\n",
    "    # linkspath ~ /dataset/migration-indicators\n",
    "    # downloadpath ~ /download/migration-indicators/0db19902-5013-42af-972d-0e5481d7ac44/Long%20term%20international%20migration.xlsx\n",
    "    import urllib.request\n",
    "    makeDir(linkspath)\n",
    "    baseurl = 'https://data.london.gov.uk/'\n",
    "    # file donwload link fdlink will look like\n",
    "    # https://data.london.gov.uk/download/migration-indicators/0db19902-5013-42af-972d-0e5481d7ac44/Long%20term%20international%20migration.xlsx\n",
    "    fdlink = baseurl + downloadpath[1:]  \n",
    "    localfile = linkspath[1:] + '/' + downloadpath.split('/')[4]\n",
    "    #print(fdlink)\n",
    "    #print(localfile)\n",
    "    urllib.request.urlretrieve(fdlink, localfile)\n",
    "    \n",
    "def getDownloadLinks():\n",
    "    # get the number of pages\n",
    "    iPagenums = getLDSDownloadLinksPageCount()\n",
    "    # initialise our links dictionary\n",
    "    links = []\n",
    "    # get the links to pages containling download file links\n",
    "    maxi = 0\n",
    "    for i in range(1, iPagenums + 1):\n",
    "        maxi = i\n",
    "        links.extend(getLDSDownloadLinks(i))\n",
    "    print(\"Scraped\", maxi, \"page links\")\n",
    "    # 3. Create a new entry in our links dictionary, \n",
    "    #    consisting of another dictionary with all the available files for download (pdf, xls, etc)\n",
    "    for i in range (0, len(links)):\n",
    "        links[i]['fileurls'] = getLDSFileDownloadLinks(links[i]['href'])\n",
    "    print(\"Scraped file download links\")\n",
    "    return links\n",
    "\n",
    "def saveDownloadLinksToJSON(links):\n",
    "    myjsonfile = jsonifier(links, \"links\")\n",
    "    f = open('lds-links.json', 'wb')\n",
    "    f.write(myjsonfile)\n",
    "    print(\"Wrote LDS download links json lds-links.json file to disk.\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London datastore download scraper started: 2018-12-04 16:18:54\n",
      "Downloaded 80 page links\n",
      "Error occured parsing file download links for href = /dataset/curio-canopy-cover-geodatabase\n",
      "Error occured parsing file download links for href = /dataset/addressbase-plus-for-contractors\n",
      "Downloaded page link files\n",
      "Wrote LDS download links json lds-links.json file to disk.\n",
      "/dataset/migration-indicators\n",
      "/download/migration-indicators/0db19902-5013-42af-972d-0e5481d7ac44/Long%20term%20international%20migration.xlsx\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'download/migration-indicators/0db19902-5013-42af-972d-0e5481d7ac44/Long%20term%20international%20migration.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-51326f0c5503>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fileurls'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckXls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fileurl'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mdownloadFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fileurl'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Timestamp end download link scraping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-43b091ea869b>\u001b[0m in \u001b[0;36mdownloadFile\u001b[1;34m(url, localPath)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mbaseurl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://data.london.gov.uk/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[0mlinkToFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseurl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinkToFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalPath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m     \u001b[1;31m# print(\"urllib.request.urlretrieve(\", linkToFile, \",\" , localPath[1:], \")\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;31m# Handle temporary file setup.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'download/migration-indicators/0db19902-5013-42af-972d-0e5481d7ac44/Long%20term%20international%20migration.xlsx'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Download London Datastore .xls(x) files\n",
    "# Timestamp start\n",
    "now = datetime.datetime.now()\n",
    "print (\"London datastore download scraper started:\", now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# get file download links\n",
    "links = getDownloadLinks()\n",
    "\n",
    "# save json file for future reference\n",
    "saveDownloadLinksToJSON(links)\n",
    "\n",
    "# file download count\n",
    "k = 0\n",
    "# iterate through list of dictionaries\n",
    "for i in range (0, len(links)):\n",
    "    # and check the download links dictionary within dictionary\n",
    "    for link in links[i]['fileurls']:\n",
    "        if(checkXls(link['fileurl'])):\n",
    "            k = k + 1\n",
    "            downloadFile(links[i]['href'], link['fileurl'])\n",
    "\n",
    "print(\"Downloaded\", k, \".xls(x) files\")\n",
    "\n",
    "# Timestamp end download link scraping\n",
    "now = datetime.datetime.now()\n",
    "print (\"London datastore download scraper scraper ended:\", now.strftime(\"%Y-%m-%d %H:%M:%S\"))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def downloadFile2(url, localPath):\n",
    "def downloadFile2(linkspath, downloadpath):\n",
    "    # expected values\n",
    "    # linkspath ~ /dataset/migration-indicators\n",
    "    # downloadpath ~ /download/migration-indicators/0db19902-5013-42af-972d-0e5481d7ac44/Long%20term%20international%20migration.xlsx\n",
    "    import urllib.request\n",
    "    makeDir(linkspath)\n",
    "    baseurl = 'https://data.london.gov.uk/'\n",
    "    # file donwload link fdlink will look like\n",
    "    # https://data.london.gov.uk/download/migration-indicators/0db19902-5013-42af-972d-0e5481d7ac44/Long%20term%20international%20migration.xlsx\n",
    "    fdlink = baseurl + downloadpath[1:]  \n",
    "    # local file path and name localfile will look like\n",
    "    # dataset/migration-indicators/Long%20term%20international%20migration.xlsx\n",
    "    localfile = linkspath[1:] + '/' + downloadpath.split('/')[4]\n",
    "    #print(fdlink)\n",
    "    #print(localfile)\n",
    "    urllib.request.urlretrieve(fdlink, localfile)\n",
    "    \n",
    "# iterate through list of dictionaries\n",
    "# for i in range (0, len(links)):\n",
    "for i in range (0, 1):\n",
    "    # and check the download links dictionary within dictionary\n",
    "    for link in links[i]['fileurls']:\n",
    "        if(checkXls(link['fileurl'])):\n",
    "            downloadFile(links[i]['href'], link['fileurl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an aside, links could be rebuilt from json file\n",
    "# download excel files\n",
    "# Validate links rebuilding from json file\n",
    "# import json\n",
    "# f = open('lds-links.json', 'r')\n",
    "# data = json.load(f)\n",
    "# links = data['links']\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras, sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-51bc4be01b49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# /dataset/diversity-london-report-data/66ee75f0-3424-4333-8ebf-d227bc74b562/diversity-in-london-data.xlsx'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# downloadlink = downloadlink.split('/')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdownloadFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "# this is the path on website where download link is to be found e.g.\n",
    "# https://data.london.gov.uk/dataset/diversity-london-report-data\n",
    "path = '/dataset/diversity-london-report-data'\n",
    "# note we will use the same path locally to store the downloaded file\n",
    "\n",
    "# this is the file donwload location e.g.\n",
    "# https://data.london.gov.uk/download/diversity-london-report-data/66ee75f0-3424-4333-8ebf-d227bc74b562/diversity-in-london-data.xlsx\n",
    "downloadlink = '/download/diversity-london-report-data/66ee75f0-3424-4333-8ebf-d227bc74b562/diversity-in-london-data.xlsx'\n",
    "# So we check if the file exists locally e.g.\n",
    "# /dataset/diversity-london-report-data/66ee75f0-3424-4333-8ebf-d227bc74b562/diversity-in-london-data.xlsx'\n",
    "# downloadlink = downloadlink.split('/')\n",
    "downloadFile(url, localPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'download/medium-term-economic-forecast/ec246c96-7e35-4661-820f-da01ab123d08/gla-london-economic-outlook-2015-11.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-06e0612ddbdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlocalPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/dataset/medium-term-economic-forecast'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/download/medium-term-economic-forecast/ec246c96-7e35-4661-820f-da01ab123d08/gla-london-economic-outlook-2015-11.xls'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdownloadFile2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-ae40030371cd>\u001b[0m in \u001b[0;36mdownloadFile2\u001b[1;34m(linkspath, downloadpath)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# file donwload link\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfdlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseurl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdownloadpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinkspath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m# print(\"urllib.request.urlretrieve(\", linkToFile, \",\" , localPath[1:], \")\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;31m# Handle temporary file setup.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'download/medium-term-economic-forecast/ec246c96-7e35-4661-820f-da01ab123d08/gla-london-economic-outlook-2015-11.xls'"
     ]
    }
   ],
   "source": [
    "# another sanity check\n",
    "localPath = '/dataset/medium-term-economic-forecast'\n",
    "url = '/download/medium-term-economic-forecast/ec246c96-7e35-4661-820f-da01ab123d08/gla-london-economic-outlook-2015-11.xls'\n",
    "downloadFile2(url, localPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/diversity-london-report-data'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/diversity-london-report-data/diversity-in-london-data.xlsx'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localDowloadPath = path + '/' + downloadlink.split('/')[4]\n",
    "localDowloadPath[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diversity-in-london-data.xlsx'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloadlink.split('/')[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
