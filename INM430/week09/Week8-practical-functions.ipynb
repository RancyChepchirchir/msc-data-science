{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INM430 Week09 City Uni courses scraper - helper functions\n",
    "\n",
    "def jsonifier(courses, toplevel):\n",
    "    import json\n",
    "    jsonified = \"{\\\"\" + toplevel + \"\\\":\" + json.dumps(results) + \"}\"\n",
    "    # to keep return type consistent with xmlifier\n",
    "    return str.encode(jsonified)\n",
    "\n",
    "def xmlifier(courses, toplevel):\n",
    "    # courses - list of dictionaries\n",
    "    # toplevel - xml doc root level\n",
    "    from xml.etree.ElementTree import Element, SubElement, Comment, tostring\n",
    "    from xml.dom import minidom\n",
    "    # since we want to generate a single XML file, we start with the root\n",
    "    # before running the loop.\n",
    "    root = Element(toplevel)  \n",
    "    keys = courses[0].keys()\n",
    "    #print(keys)\n",
    "    for i in range(len(courses)):\n",
    "        #\"name\" : courseName,\n",
    "        #\"description\" : courseDescription,\n",
    "        #\"school\" : courseSchool,\n",
    "        #\"code\" : courseCode,\n",
    "        #\"level\" : level,\n",
    "        # And let's start populating the XML file by adding the scraped data one by one\n",
    "        rootXML = SubElement(root, 'Course') # hardcoded, TODO abstract\n",
    "        #print(i)         \n",
    "        for key in keys:\n",
    "            detailXML = SubElement(rootXML, key)\n",
    "            detailXML.text = results[i][key]\n",
    "            #print (results[i][key]) \n",
    "    xmlified = tostring(root, 'utf-8')\n",
    "    return xmlified\n",
    "    \n",
    "def getCityCourseCount(course):\n",
    "    # Find the number of City University of London courses shown on website\n",
    "    # by scraping tag course-finder__results__summary\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur\n",
    "    urlToScrape = \"https://www.city.ac.uk/courses?level=\" + course\n",
    "    r = ur.urlopen(urlToScrape).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    pageCount = soup.find_all('div', attrs={'class': 'course-finder__results__summary'})\n",
    "    for courseListItem in pageCount:\n",
    "        try:\n",
    "            # we are expecting a span tag like this \"<span>67 search results</span>\"            \n",
    "            myspan = courseListItem.find('span').text\n",
    "            # we've now stripped the tags and have \"67 search results\"\n",
    "            # we split the string into an array, using the default white space delimiter\n",
    "            # and take the zeroeth element which is 67\n",
    "            mycount = myspan.split()[0]\n",
    "        except Exception as e:\n",
    "            mycount = \"0\" # no results\n",
    "    return mycount\n",
    "\n",
    "def getCourses(level, count):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request as ur    \n",
    "    import re # regular expressions\n",
    "    i = 1\n",
    "    step = 10\n",
    "    results = []\n",
    "    while i <= int(count):\n",
    "        # we know that the general URL format will be\n",
    "        # https://www.city.ac.uk/courses?level=(level)&p=(n)\n",
    "        # where paging is requested in querystring (text to the right of question mark)\n",
    "        # attribute p, where (n) represents the starting number\n",
    "        # we know that paging increments in counts of 10 i.e. 1, 11, 21 and so forth\n",
    "        # hence we count up to 61, the url then requests pages starting from 61, we know\n",
    "        # that our count for undergraduate courses was 67 can we can stop there\n",
    "        # url = \"https://www.city.ac.uk/courses?level=Undergraduate&p=\" + str(i)\n",
    "        url = \"https://www.city.ac.uk/courses?level=\" + level + \"&p=\" + str(i)\n",
    "        r = ur.urlopen(url).read()\n",
    "        soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "        # get all the \"div\"s that ar of the class we are interested in.\n",
    "        # note that the following line will return us a collection of \"div\" sections\n",
    "        # courseList = soup.find_all('div', attrs={'class': 'course-finder__results__item course-finder__results__item--undergraduate'})\n",
    "        # Let's use regular expressions wildcard instead\n",
    "        courseList = soup.find_all('div', attrs={'class': re.compile('course-finder__results__item course-finder__results__item--.*')})\n",
    "        \n",
    "        for courseListItem in courseList:\n",
    "\n",
    "            # Here we cover everything with try/except constructs to ensure that we are not failing when an element is not there.    \n",
    "            try:\n",
    "                # first go inside the DIV to access the course name, note that it is under a <a> tag, so that's where we need to access here.\n",
    "                courseNameElement = courseListItem.find('div', attrs={'class': \"col-sm-24 col-md-18 col-lg-20\"})\n",
    "                courseName = courseNameElement.find('a').text\n",
    "\n",
    "            except Exception as e:\n",
    "                courseName = \"\"\n",
    "\n",
    "            try:\n",
    "                # here, we access the course description, you can find that it is under a DIV\n",
    "                courseDescriptionElement = courseListItem.find('div', attrs={'class': \"course-finder__results__item__description\"})\n",
    "                courseDescription = courseDescriptionElement.text\n",
    "            except Exception as e:\n",
    "                courseDescription = \"\"\n",
    "\n",
    "            try:\n",
    "                # now on to scraping the name of the school offering the course, this time under an <a> tag \n",
    "                courseSchoolElement = courseListItem.find('div', attrs={'class': \"course-finder__results__item__md course-finder__results__item__md--school\"})\n",
    "                courseSchool = courseSchoolElement.find('a').text\n",
    "            except Exception as e:\n",
    "                courseSchool = \"\"\n",
    "\n",
    "            try:\n",
    "                # when we try to get the course code, we notice that there are two <span> sections under the section \n",
    "                # we are interested in, we want to get the second one, hence => .find_all('span')[1]\n",
    "\n",
    "                courseCodeElement_1 = courseListItem.find('div', attrs={'class': \"course-finder__results__item__md course-finder__results__item__md--code\"})\n",
    "                courseCodeElement_2 = courseCodeElement_1.find_all('span')[2]\n",
    "                courseCode = courseCodeElement_2.text\n",
    "            except Exception as e:\n",
    "                courseCode = \"\"\n",
    "            # let's populate a dictionary for now, and decide how to format it later\n",
    "            if len(courseName) > 0:\n",
    "                course = {\n",
    "                \"name\" : courseName,\n",
    "                \"description\" : courseDescription,\n",
    "                \"school\" : courseSchool,\n",
    "                \"code\" : courseCode,\n",
    "                \"level\" : level,\n",
    "                }\n",
    "                results.append(course)\n",
    "        # increment count\n",
    "        i += step\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City Uni scraper started: 2018-11-22 12:21:59\n",
      "Number of Undergraduate courses = 67\n",
      "Number of Postgraduate courses = 142\n",
      "Number of Research Degrees courses = 24\n",
      "Number of Foundation courses = 9\n",
      "Number of CPD courses = 251\n",
      "Number of Short Courses courses = 133\n",
      "Number of Executive Education courses = 15\n",
      "Running total = 641\n",
      "Wrote city courses xml file to disk.\n",
      "Wrote city courses json file to disk.\n",
      "City Uni scraper finished: 2018-11-22 12:30:20\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Items in citycourselist list correspond to city url query string \"level\" e.g.\n",
    "# https://www.city.ac.uk/courses?level=Executive Education\n",
    "citycourselist = [\"Undergraduate\", \"Postgraduate\", \"Research Degrees\", \"Foundation\", \"CPD\", \"Short Courses\", \"Executive Education\"]\n",
    "runningtotal = 0\n",
    "results = []\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print (\"City Uni scraper started:\", now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "for course in citycourselist:\n",
    "    mycount = getCityCourseCount(course)\n",
    "    results.extend(getCourses(course, mycount))\n",
    "    print(\"Number of\", course, \"courses =\", mycount)\n",
    "    runningtotal += int(mycount)\n",
    "print(\"Running total =\", runningtotal)\n",
    "\n",
    "myxmlfile = xmlifier(results, \"Courses\")\n",
    "f = open('CityCourses.xml', 'wb')\n",
    "f.write(myxmlfile)\n",
    "print(\"Wrote city courses xml file to disk.\")\n",
    "f.close()\n",
    "\n",
    "myjsonfile = jsonifier(results, \"Courses\")\n",
    "f = open('CityCourses.json', 'wb')\n",
    "f.write(myjsonfile)\n",
    "print(\"Wrote city courses json file to disk.\")\n",
    "f.close()\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print (\"City Uni scraper finished:\", now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
